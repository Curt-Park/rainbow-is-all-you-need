{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04. Noisy Networks for Exploration\n",
    "\n",
    "[M. Fortunato et al., \"Noisy Networks for Exploration.\" arXiv preprint arXiv:1706.10295, 2017.](https://arxiv.org/pdf/1706.10295.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "from modules.segment_tree import MinSegmentTree, SumSegmentTree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noisy Layer\n",
    "\n",
    "NoisyNet is an exploration method that learned perturbations of the network weights are used to drive exploration. The key insight is that a single change to the weight vector can induce a consistent, and potentially very complex, state-dependent change in policy over multiple time steps.\n",
    "\n",
    "Firstly, let's take a look into a linear layer of a neural network with $p$ inputs and $q$ outputs, represented by\n",
    "\n",
    "$$\n",
    "y = wx + b,\n",
    "$$\n",
    "\n",
    "where $x \\in \\mathbb{R}^p$ is the layer input, $w \\in \\mathbb{R}^{q \\times p}$, and $b \\in \\mathbb{R}$ the bias.\n",
    "\n",
    "The corresponding noisy linear layer is defined as:\n",
    "\n",
    "$$\n",
    "y = (\\mu^w + \\sigma^w \\odot \\epsilon^w) x + \\mu^b + \\sigma^b \\odot \\epsilon^b,\n",
    "$$\n",
    "\n",
    "where $\\mu^w + \\sigma^w \\odot \\epsilon^w$ and $\\mu^b + \\sigma^b \\odot \\epsilon^b$ replace $w$ and $b$ in the first linear layer equation. The parameters $\\mu^w \\in \\mathbb{R}^{q \\times p}, \\mu^b \\in \\mathbb{R}^q, \\sigma^w \\in \\mathbb{R}^{q \\times p}$ and $\\sigma^b \\in \\mathbb{R}^q$ are learnable, whereas $\\epsilon^w \\in \\mathbb{R}^{q \\times p}$ and $\\epsilon^b \\in \\mathbb{R}^q$ are noise random variables which can be generated by one of the following two ideas:\n",
    "\n",
    "1. **Independent Gaussian noise**: the noise applied to each weight and bias is independent, where each random noise entry is drawn from a unit Gaussian distribution. This means that for each noisy linear layer, there are $pq + q$ noise variables (for $p$ inputs to the layer and $q$ outputs).\n",
    "2. **Factorised Gaussian noise:** This is a more computationally efficient way. It produces 2 random Gaussian noise vectors ($p, q$) and makes $pq + q$ noise entries by outer product as follows:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\epsilon_{i,j}^w &= f(\\epsilon_i) f(\\epsilon_j),\\\\\n",
    "\\epsilon_{j}^b &= f(\\epsilon_i),\\\\\n",
    "\\text{where } f(x) &= sgn(x) \\sqrt{|x|}.\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "In all experiements of the paper, the authors used Factorised Gaussian noise, so we will go for it as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoisyLinear(nn.Module):\n",
    "    \"\"\"Noisy linear module for NoisyNet.\n",
    "    \n",
    "    References:\n",
    "        https://github.com/higgsfield/RL-Adventure/blob/master/5.noisy%20dqn.ipynb\n",
    "        https://github.com/Kaixhin/Rainbow/blob/master/model.py\n",
    "        \n",
    "    Attributes:\n",
    "        in_features (int): input size of linear module\n",
    "        out_features (int): output size of linear module\n",
    "        std_init (float): initial std value\n",
    "        weight_mu (nn.Parameter): mean value weight parameter\n",
    "        weight_sigma (nn.Parameter): std value weight parameter\n",
    "        bias_mu (nn.Parameter): mean value bias parameter\n",
    "        bias_sigma (nn.Parameter): std value bias parameter\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features: int, out_features: int, std_init: float = 0.5):\n",
    "        \"\"\"Initialization.\"\"\"\n",
    "        super(NoisyLinear, self).__init__()\n",
    "        \n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.std_init = std_init\n",
    "\n",
    "        self.weight_mu = nn.Parameter(torch.Tensor(out_features, in_features))\n",
    "        self.weight_sigma = nn.Parameter(torch.Tensor(out_features, in_features))\n",
    "        self.register_buffer(\"weight_epsilon\", torch.Tensor(out_features, in_features))\n",
    "\n",
    "        self.bias_mu = nn.Parameter(torch.Tensor(out_features))\n",
    "        self.bias_sigma = nn.Parameter(torch.Tensor(out_features))\n",
    "        self.register_buffer(\"bias_epsilon\", torch.Tensor(out_features))\n",
    "\n",
    "        self.reset_parameters()\n",
    "        self.reset_noise()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        \"\"\"Reset trainable network parameters (factorized gaussian noise).\"\"\"\n",
    "        mu_range = 1 / math.sqrt(self.in_features)\n",
    "        self.weight_mu.data.uniform_(-mu_range, mu_range)\n",
    "        self.weight_sigma.data.fill_(self.std_init / math.sqrt(self.in_features))\n",
    "        self.bias_mu.data.uniform_(-mu_range, mu_range)\n",
    "        self.bias_sigma.data.fill_(self.std_init / math.sqrt(self.out_features))\n",
    "\n",
    "    def reset_noise(self):\n",
    "        \"\"\"Make new noise.\"\"\"\n",
    "        epsilon_in = self.scale_noise(self.in_features)\n",
    "        epsilon_out = self.scale_noise(self.out_features)\n",
    "\n",
    "        # outer product\n",
    "        self.weight_epsilon.copy_(epsilon_out.ger(epsilon_in))\n",
    "        self.bias_epsilon.copy_(epsilon_out)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward method implementation.\n",
    "        \n",
    "        We don't use separate statements on train / eval mode.\n",
    "        It doesn't show remarkable difference of performance.\n",
    "        \"\"\"\n",
    "        return F.linear(\n",
    "            x,\n",
    "            self.weight_mu + self.weight_sigma * self.weight_epsilon,\n",
    "            self.bias_mu + self.bias_sigma * self.bias_epsilon,\n",
    "        )\n",
    "    \n",
    "    @staticmethod\n",
    "    def scale_noise(size: int) -> torch.Tensor:\n",
    "        \"\"\"Set scale to make noise (factorized gaussian noise).\"\"\"\n",
    "        x = torch.FloatTensor(np.random.normal(loc=0.0, scale=1.0, size=size))\n",
    "\n",
    "        return x.sign().mul(x.abs().sqrt())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NoisyNet based on DuelingNet (including Double-DQN and PER)\n",
    "\n",
    "In the paper, NoisyNet is used as a component of the Dueling Network Architecture, which includes Double-DQN and Prioritized Experience Replay."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dueling Network\n",
    "\n",
    "The proposed network architecture, which is named *dueling architecture*, explicitly separates the representation of state values and (state-dependent) action advantages. \n",
    "\n",
    "![fig1](https://user-images.githubusercontent.com/14961526/60322956-c2f0b600-99bb-11e9-9ed4-443bd14bc3b0.png)\n",
    "\n",
    "The dueling network automatically produces separate estimates of the state value function and advantage function, without any extra supervision. Intuitively, the dueling architecture can learn which states are (or are not) valuable, without having to learn the effect of each action for each state. This is particularly useful in states where its actions do not affect the environment in any relevant way. \n",
    "\n",
    "The dueling architecture represents both the value $V(s)$ and advantage $A(s, a)$ functions with a single deep model whose output combines the two to produce a state-action value $Q(s, a)$. Unlike in advantage updating, the representation and algorithm are decoupled by construction.\n",
    "\n",
    "$$A^\\pi (s, a) = Q^\\pi (s, a) - V^\\pi (s).$$\n",
    "\n",
    "The value function $V$ measures the how good it is to be in a particular state $s$. The $Q$ function, however, measures the the value of choosing a particular action when in this state. Now, using the definition of advantage, we might be tempted to construct the aggregating module as follows:\n",
    "\n",
    "$$Q(s, a; \\theta, \\alpha, \\beta) = V (s; \\theta, \\beta) + A(s, a; \\theta, \\alpha),$$\n",
    "\n",
    "where $\\theta$ denotes the parameters of the convolutional layers, while $\\alpha$ and $\\beta$ are the parameters of the two streams of fully-connected layers.\n",
    "\n",
    "Unfortunately, the above equation is unidentifiable in the sense that given $Q$ we cannot recover $V$ and $A$ uniquely; for example, there are uncountable pairs of $V$ and $A$ that make $Q$ values to zero. To address this issue of identifiability, we can force the advantage function estimator to have zero advantage at the chosen action. That is, we let the last module of the network implement the forward mapping.\n",
    "\n",
    "$$\n",
    "Q(s, a; \\theta, \\alpha, \\beta) = V (s; \\theta, \\beta) + \\big( A(s, a; \\theta, \\alpha) - \\max_{a' \\in |\\mathcal{A}|} A(s, a'; \\theta, \\alpha) \\big).\n",
    "$$\n",
    "\n",
    "This formula guarantees that we can recover the unique $V$ and $A$, but the optimization is not so stable because the advantages have to compensate any change to the optimal action’s advantage. Due to the reason, an alternative module that replaces the max operator with an average is proposed:\n",
    "\n",
    "$$\n",
    "Q(s, a; \\theta, \\alpha, \\beta) = V (s; \\theta, \\beta) + \\big( A(s, a; \\theta, \\alpha) - \\frac{1}{|\\mathcal{A}|} \\sum_{a'} A(s, a'; \\theta, \\alpha) \\big).\n",
    "$$\n",
    "\n",
    "Unlike the max advantage form, in this formula, the advantages only need to change as fast as the mean, so it increases the stability of optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self, in_dim: int, out_dim: int):\n",
    "        \"\"\"Initialization.\"\"\"\n",
    "        super(Network, self).__init__()\n",
    "\n",
    "        # set common feature layer\n",
    "        self.feature_layer = nn.Sequential(\n",
    "            nn.Linear(in_dim, 128), \n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        # set advantage layers\n",
    "        self.advantage_hidden_layer = NoisyLinear(128, 128)\n",
    "        self.advantage_out_layer = NoisyLinear(128, out_dim)\n",
    "\n",
    "        # set value layer\n",
    "        self.value_hidden_layer = NoisyLinear(128, 128)\n",
    "        self.value_out_layer = NoisyLinear(128, 1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward method implementation.\"\"\"\n",
    "        feature = self.feature_layer(x)\n",
    "        \n",
    "        val_x = F.relu(self.value_hidden_layer(feature))\n",
    "        adv_x = F.relu(self.advantage_hidden_layer(feature))\n",
    "        \n",
    "        value = self.value_out_layer(val_x)\n",
    "        advantage = self.advantage_out_layer(adv_x)\n",
    "\n",
    "        q = value + advantage - advantage.mean(dim=-1, keepdim=True)\n",
    "        \n",
    "        return q\n",
    "    \n",
    "    def reset_noise(self):\n",
    "        \"\"\"Reset noise.\"\"\"\n",
    "        self.advantage_hidden_layer.reset_noise()\n",
    "        self.advantage_out_layer.reset_noise()\n",
    "        self.value_hidden_layer.reset_noise()\n",
    "        self.value_out_layer.reset_noise()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay buffer\n",
    "\n",
    "Typically, people implement replay buffers with one of the following three data structures:\n",
    "\n",
    "  - collections.deque\n",
    "  - list\n",
    "  - numpy.ndarray\n",
    "  \n",
    "**deque** is very easy to handle once you initialize its maximum length (e.g. deque(maxlen=buffer_size)). However, the indexing operation of deque gets terribly slow as it grows up because it is [internally doubly linked list](https://wiki.python.org/moin/TimeComplexity#collections.deque). On the other hands, **list** is an array, so it is relatively faster than deque when you sample batches at every step. Its amortized cost of  *Get item* is [O(1)](https://wiki.python.org/moin/TimeComplexity#list).\n",
    "\n",
    "Last but not least, let's see **numpy.ndarray**. numpy.ndarray is even faster than list due to the fact that it is [a homogeneous array of fixed-size items](https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray), so you can get the benefits of [locality of reference](https://en.wikipedia.org/wiki/Locality_of_reference). Whereas list is an array of pointers to objects, even when all of them are of the same type.\n",
    "\n",
    "Here, we are going to implement a replay buffer using numpy.ndarray.\n",
    "\n",
    "\n",
    "Reference: [OpenAI spinning-up](https://github.com/openai/spinningup/blob/master/spinup/algos/sac/sac.py#L10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"A simple numpy replay buffer.\"\"\"\n",
    "\n",
    "    def __init__(self, obs_dim: int, size: int, batch_size: int = 32):\n",
    "        self.obs_buf = np.zeros([size, obs_dim], dtype=np.float32)\n",
    "        self.next_obs_buf = np.zeros([size, obs_dim], dtype=np.float32)\n",
    "        self.acts_buf = np.zeros([size, 1], dtype=np.float32)\n",
    "        self.rews_buf = np.zeros([size, 1], dtype=np.float32)\n",
    "        self.done_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.ptr, self.size, self.max_size, self.batch_size = 0, 0, size, batch_size\n",
    "\n",
    "    def store(self, obs: np.ndarray, act: int, rew: float, next_obs: np.ndarray, done: bool):\n",
    "        self.obs_buf[self.ptr] = obs\n",
    "        self.next_obs_buf[self.ptr] = next_obs\n",
    "        self.acts_buf[self.ptr] = act\n",
    "        self.rews_buf[self.ptr] = rew\n",
    "        self.done_buf[self.ptr] = done\n",
    "        self.ptr = (self.ptr + 1) % self.max_size\n",
    "        self.size = min(self.size + 1, self.max_size)\n",
    "\n",
    "    def sample_batch(self) -> Dict[str, np.ndarray]:\n",
    "        idxs = np.random.choice(self.size, size=self.batch_size, replace=False)\n",
    "        return dict(obs=self.obs_buf[idxs],\n",
    "                    next_obs=self.next_obs_buf[idxs],\n",
    "                    acts=self.acts_buf[idxs],\n",
    "                    rews=self.rews_buf[idxs],\n",
    "                    done=self.done_buf[idxs])\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prioritized replay Buffer\n",
    "\n",
    "Using a replay memory leads to design choices at two levels: which experiences to store, and which experiences to replay (and how to do so). This paper addresses only the latter: making the most effective use of the replay memory for learning, assuming that its contents are outside of our control.\n",
    "\n",
    "The central component of prioritized replay is the criterion by which the importance of each transition is measured. A reasonable approach is to use the magnitude of a transition’s TD error $\\delta$, which indicates how ‘surprising’\n",
    "or unexpected the transition is. This algorithm stores the last encountered TD error along with each transition in the replay memory. The transition with the largest absolute TD error is replayed from the memory. A Q-learning update\n",
    "is applied to this transition, which updates the weights in proportion to the TD error. One thing to note that new transitions arrive without a known TD-error, so it puts them at maximal priority in order to guarantee that all experience is seen at least once. (see *store* method)\n",
    "\n",
    "We might use 2 ideas to deal with TD-error: 1. greedy TD-error prioritization, 2. stochastic prioritization. However, greedy TD-error prioritization has a severe drawback. Greedy prioritization focuses on asmall subset of the experience: errors shrink slowly, especially when using function approximation, meaning that the initially high error transitions get replayed frequently. This lack of diversity that makes the system prone to over-fitting. To overcome this issue, we will use a stochastic sampling method that interpolates between pure greedy prioritization and uniform random sampling.\n",
    "\n",
    "$$\n",
    "P(i) = \\frac{p_i^{\\alpha}}{\\sum_k p_k^{\\alpha}}\n",
    "$$\n",
    "\n",
    "where $p_i > 0$ is the priority of transition $i$. The exponent $\\alpha$ determines how much prioritization is used, with $\\alpha = 0$ corresponding to the uniform case. In practice, we use additional term $\\epsilon$ in order to guarantee all transactions can be possibly sampled: $p_i = |\\delta_i| + \\epsilon$, where $\\epsilon$ is a small positive constant.\n",
    "\n",
    "One more. Let's recall one of the main ideas of DQN. To remove correlation of observations, it uses uniformly random sampling from the replay buffer. Prioritized replay introduces bias because it doesn't sample experiences uniformly at random due to the sampling proportion correspoding to TD-error. We can correct this bias by using importance-sampling (IS) weights\n",
    "\n",
    "$$\n",
    "w_i = \\big( \\frac{1}{N} \\cdot \\frac{1}{P(i)} \\big)^\\beta\n",
    "$$\n",
    "\n",
    "that fully compensates for the non-uniform probabilities $P(i)$ if $\\beta = 1$. These weights can be folded into the Q-learning update by using $w_i\\delta_i$ instead of $\\delta_i$. In typical reinforcement learning scenarios, the unbiased nature of the updates is most important near convergence at the end of training, We therefore exploit the flexibility of annealing the amount of importance-sampling correction over time, by defining a schedule on the exponent $\\beta$ that reaches 1 only at the end of learning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrioritizedReplayBuffer(ReplayBuffer):\n",
    "    \"\"\"Prioritized Replay buffer.\n",
    "    \n",
    "    Attributes:\n",
    "        max_priority (float): max priority\n",
    "        tree_ptr (int): next index of tree\n",
    "        alpha (float): alpha parameter for prioritized replay buffer\n",
    "        sum_tree (SumSegmentTree): sum tree for prior\n",
    "        min_tree (MinSegmentTree): min tree for min prior to get max weight\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, obs_dim: int, size: int, batch_size: int = 32, alpha: float = 0.6):\n",
    "        \"\"\"Initialization.\"\"\"\n",
    "        assert alpha >= 0\n",
    "        \n",
    "        super(PrioritizedReplayBuffer, self).__init__(obs_dim, size, batch_size)\n",
    "        self.max_priority, self.tree_ptr = 1.0, 0\n",
    "        self.alpha = alpha\n",
    "        \n",
    "        # capacity must be positive and a power of 2.\n",
    "        tree_capacity = 1\n",
    "        while tree_capacity < self.max_size:\n",
    "            tree_capacity *= 2\n",
    "\n",
    "        self.sum_tree = SumSegmentTree(tree_capacity)\n",
    "        self.min_tree = MinSegmentTree(tree_capacity)\n",
    "        \n",
    "    def store(self, obs: np.ndarray, act: int, rew: float, next_obs: np.ndarray, done: bool):\n",
    "        \"\"\"Store experience and priority.\"\"\"\n",
    "        super().store(obs, act, rew, next_obs, done)\n",
    "        \n",
    "        self.sum_tree[self.tree_ptr] = self.max_priority ** self.alpha\n",
    "        self.min_tree[self.tree_ptr] = self.max_priority ** self.alpha\n",
    "        self.tree_ptr = (self.tree_ptr + 1) % self.max_size\n",
    "\n",
    "    def sample_batch(self, beta: float = 0.4) -> Tuple[torch.Tensor, ...]:\n",
    "        \"\"\"Sample a batch of experiences.\"\"\"\n",
    "        assert len(self) >= self.batch_size\n",
    "        assert beta > 0\n",
    "        \n",
    "        indices = self._sample_proportional()\n",
    "        \n",
    "        obs = self.obs_buf[indices]\n",
    "        next_obs = self.next_obs_buf[indices]\n",
    "        acts = self.acts_buf[indices]\n",
    "        rews = self.rews_buf[indices]\n",
    "        done = self.done_buf[indices]\n",
    "        weights = np.array([self._calculate_weight(i, beta) for i in indices])\n",
    "        \n",
    "        return dict(\n",
    "            obs=obs,\n",
    "            next_obs=next_obs,\n",
    "            acts=acts,\n",
    "            rews=rews,\n",
    "            done=done,\n",
    "            weights=weights,\n",
    "            indices=indices,\n",
    "        )\n",
    "        \n",
    "    def update_priorities(self, indices: List[int], priorities: np.ndarray):\n",
    "        \"\"\"Update priorities of sampled transitions.\"\"\"\n",
    "        assert len(indices) == len(priorities)\n",
    "\n",
    "        for idx, priority in zip(indices, priorities):\n",
    "            assert priority > 0\n",
    "            assert 0 <= idx < len(self)\n",
    "\n",
    "            self.sum_tree[idx] = priority ** self.alpha\n",
    "            self.min_tree[idx] = priority ** self.alpha\n",
    "\n",
    "            self.max_priority = max(self.max_priority, priority)\n",
    "            \n",
    "    def _sample_proportional(self) -> List[int]:\n",
    "        \"\"\"Sample indices based on proportions.\"\"\"\n",
    "        indices = []\n",
    "        p_total = self.sum_tree.sum(0, len(self) - 1)\n",
    "        segment = p_total / self.batch_size\n",
    "        \n",
    "        for i in range(self.batch_size):\n",
    "            a = segment * i\n",
    "            b = segment * (i + 1)\n",
    "            upperbound = random.uniform(a, b)\n",
    "            idx = self.sum_tree.retrieve(upperbound)\n",
    "            indices.append(idx)\n",
    "            \n",
    "        return indices\n",
    "    \n",
    "    def _calculate_weight(self, idx: int, beta: float):\n",
    "        \"\"\"Calculate the weight of the experience at idx.\"\"\"\n",
    "        # get max weight\n",
    "        p_min = self.min_tree.min() / self.sum_tree.sum()\n",
    "        max_weight = (p_min * len(self)) ** (-beta)\n",
    "        \n",
    "        # calculate weights\n",
    "        p_sample = self.sum_tree[idx] / self.sum_tree.sum()\n",
    "        weight = (p_sample * len(self)) ** (-beta)\n",
    "        weight = weight / max_weight\n",
    "        \n",
    "        return weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN Agent\n",
    "\n",
    "Here is a summary of DQNAgent class.\n",
    "\n",
    "| Method           | Note                                                 |\n",
    "| ---              | ---                                                  |\n",
    "|select_action     | select an action from the input state.               |\n",
    "|step              | take an action and return the response of the env.   |\n",
    "|compute_dqn_loss  | return dqn loss.                                     |\n",
    "|update_model      | update the model by gradient descent.                |\n",
    "|target_hard_update| hard update from the local model to the target model.|\n",
    "|train             | train the agent during num_frames.                   |\n",
    "|test              | test the agent (1 episode).                          |\n",
    "|plot              | plot the training progresses.                        |\n",
    "\n",
    "Reinforcement learning is known to be unstable or even to diverge when a nonlinear function approximator such as a neural network is used to represent the action-value (also known as $Q$) function. This instability has several causes: the correlations present in the sequence of observations, the fact that small updates to $Q$ may significantly change the policy and therefore change the data distribution, and the correlations between the action-values ($Q$) and the target values $r + \\gamma \\max_{a'} Q(s', a')$.\n",
    "\n",
    "The authors suggest two key ideas to address these instabilities with a novel variant of Q-learning: Replay buffer and Fixed Q-target.\n",
    "\n",
    "#### Uniformly random sampling from Experience Replay Memory\n",
    "\n",
    "Reinforcement learning agent stores the experiences consecutively in the buffer, so adjacent ($s, a, r, s'$) transitions stored are highly likely to have correlation. To remove this, the agent samples experiences uniformly at random from the pool of stored samples $\\big( (s, a, r, s') \\sim U(D) \\big)$. See sample_batch method of ReplayBuffer class for more details.\n",
    "\n",
    "#### Fixed Q-target\n",
    "\n",
    "DQN uses an iterative update that adjusts the action-values ($Q$) towards target values that are only periodically updated, thereby reducing correlations with the target; if not, it is easily divergy because the target continuously moves. The Q-learning update at iteration $i$ uses the following loss function:\n",
    "\n",
    "$$\n",
    "L_i(\\theta_i) = \\mathbb{E}_{(s,a,r,s') \\sim U(D)} \\big[ \\big( r + \\gamma \\max_{a'} Q(s',a';\\theta_i^-) - Q(s, a; \\theta_i) \\big)^2 \\big]\n",
    "$$\n",
    "\n",
    "in which $\\gamma$ is the discount factor determining the agent’s horizon, $\\theta_i$ are the parameters of the Q-network at iteration $i$ and $\\theta_i^-$ are the network parameters used to compute the target at iteration $i$. The target network parameters $\\theta_i^-$ are only updated with the Q-network parameters ($\\theta_i$) every C steps and are held fixed between individual updates. ($C = 200$ in CartPole-v0)\n",
    "\n",
    "#### For more stability: Gradient clipping\n",
    "\n",
    "The authors also found it helpful to clip the error term from the update $r + \\gamma \\max_{a'} Q(s', a'; \\theta_i^-) - Q(s,a,;\\theta_i)$ to be between -1 and 1. Because the absolute value loss function $|x|$ has a derivative of -1 for all negative values of x and a derivative of 1 for all positive values of x, clipping the squared error to be between -1 and 1 corresponds to using an absolute value loss function for errors outside of the (-1,1) interval. This form of error clipping further improved the stability of the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Double DQN\n",
    "\n",
    "\n",
    "\n",
    "Let's take a close look at the difference between DQN and Double-DQN. The max operator in standard Q-learning and DQN uses the same values both to select and to evaluate an action. This makes it more likely to select overestimated values, resulting in overoptimistic value estimates.\n",
    "\n",
    "$$\n",
    "\\theta_{t+1} = \\theta_t + \\alpha \\big(Y_t^Q - Q(S_t, A_t; \\theta_t)\\big) \\cdot \\nabla_{\\theta_t} Q(S_t, A_t; \\theta_t),\\\\\n",
    "\\text{where } \\alpha \\text{ is a scalar step size and the target } Y_t^Q \\text{is defined as }\\\\\n",
    "Y_t^Q = R_{t+1} + \\gamma \\max_a Q(S_{t+1}, a; \\theta_t).\n",
    "$$\n",
    "\n",
    "In Double Q-learning ([van Hasselt 2010](https://papers.nips.cc/paper/3964-double-q-learning.pdf)), two value functions are learned by assigning experiences randomly to update one of the two value functions, resulting in two sets of weights, $\\theta$ and $\\theta'$. For each update, one set of weights is used to determine the greedy policy and the other to determine its value. For a clear comparison, we can untangle the selection and evaluation in Q-learning and rewrite DQN's target as\n",
    "\n",
    "$$\n",
    "Y_t^Q = R_{t+1} + \\gamma \\max_a Q(S_{t+1}, \\arg\\max_a Q(S_{t+1}, a; \\theta_t); \\theta_t).\n",
    "$$\n",
    "\n",
    "The Double Q-learning error can then be written as\n",
    "\n",
    "$$\n",
    "Y_t^{DoubleQ} = R_{t+1} + \\gamma \\max_a Q(S_{t+1}, \\arg\\max_a Q(S_{t+1}, a; \\theta_t); \\theta_t').\n",
    "$$\n",
    "\n",
    "The idea of Double Q-learning is to reduce overestimations by decomposing the max operation in the target into action selection and action evaluation. Although not fully decoupled, the target network in the DQN architecture provides a natural candidate for the second value function, without having to introduce additional networks. In conclusion, the weights of the second network $\\theta_t'$ are replaced with the weights of the target network for the evaluation of the current greedy policy. This makes just a small change in calculating the target value of DQN loss.\n",
    "\n",
    "##### DQN:\n",
    "\n",
    "```\n",
    "target = reward + gamma * dqn_target(next_state).max(dim=1, keepdim=True)[0]\n",
    "```\n",
    "\n",
    "##### DoubleDQN:\n",
    "\n",
    "```\n",
    "selected_action = dqn(next_state).argmax(dim=1, keepdim=True)\n",
    "target = reward + gamma * dqn_target(next_state).gather(1, selected_action)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN + NoisyNet\n",
    "\n",
    "NoisyNet is an alternertive to $\\epsilon$-greedy method, so all $\\epsilon$ related lines are removed. Please check all comments with *NoisyNet*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    \"\"\"DQN Agent interacting with environment.\n",
    "    \n",
    "    Attribute:\n",
    "        env (gym.Env): openAI Gym environment\n",
    "        memory (ReplayBuffer): replay memory to store transitions\n",
    "        batch_size (int): batch size for sampling\n",
    "        target_update (int): period for target model's hard update\n",
    "        gamma (float): discount factor\n",
    "        dqn (Network): model to train and select actions\n",
    "        dqn_target (Network): target model to update\n",
    "        optimizer (torch.optim): optimizer for training dqn\n",
    "        transition (list): transition information including state, action, reward, next_state, done\n",
    "        beta (float): determines how much importance sampling is used\n",
    "        prior_eps (float): guarantees every transition can be sampled\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        env: gym.Env,\n",
    "        memory_size: int,\n",
    "        batch_size: int,\n",
    "        target_update: int,\n",
    "        gamma: float = 0.99,\n",
    "        alpha: float = 0.6,\n",
    "        beta: float = 0.4,\n",
    "        prior_eps: float = 1e-6,\n",
    "    ):\n",
    "        \"\"\"Initialization.\n",
    "        \n",
    "        Args:\n",
    "            env (gym.Env): openAI Gym environment\n",
    "            memory_size (int): length of memory\n",
    "            batch_size (int): batch size for sampling\n",
    "            target_update (int): period for target model's hard update\n",
    "            lr (float): learning rate\n",
    "            gamma (float): discount factor\n",
    "            alpha (float): determines how much prioritization is used\n",
    "            beta (float): determines how much importance sampling is used\n",
    "            prior_eps (float): guarantees every transition can be sampled\n",
    "        \"\"\"\n",
    "        # NoisyNet: All attributes related to epsilon are removed\n",
    "        obs_dim = env.observation_space.shape[0]\n",
    "        action_dim = env.action_space.n\n",
    "        self.env = env\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.target_update = target_update\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        # device: cpu / gpu\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(self.device)\n",
    "        \n",
    "        # PER\n",
    "        # In DQN, We used \"ReplayBuffer(obs_dim, memory_size, batch_size)\"\n",
    "        self.beta = beta\n",
    "        self.prior_eps = prior_eps\n",
    "        self.memory = PrioritizedReplayBuffer(obs_dim, memory_size, batch_size, alpha)\n",
    "\n",
    "        # networks: dqn, dqn_target\n",
    "        self.dqn = Network(obs_dim, action_dim).to(self.device)\n",
    "        self.dqn_target = Network(obs_dim, action_dim).to(self.device)\n",
    "        self.dqn_target.load_state_dict(self.dqn.state_dict())\n",
    "        self.dqn_target.eval()\n",
    "        \n",
    "        # optimizer\n",
    "        self.optimizer = optim.Adam(self.dqn.parameters())\n",
    "\n",
    "        # transition to store in memory\n",
    "        self.transition = list()\n",
    "        \n",
    "        # mode: train / test\n",
    "        self.is_test = False\n",
    "\n",
    "    def select_action(self, state: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Select an action from the input state.\"\"\"\n",
    "        # NoisyNet: no epsilon greedy action selection\n",
    "        selected_action = self.dqn(\n",
    "            torch.FloatTensor(state).to(self.device)\n",
    "        ).argmax()\n",
    "        selected_action = selected_action.detach().cpu().numpy()\n",
    "        \n",
    "        if not self.is_test:\n",
    "            self.transition = [state, selected_action]\n",
    "        \n",
    "        return selected_action\n",
    "\n",
    "    def step(self, action: np.ndarray) -> Tuple[np.ndarray, np.float64, bool]:\n",
    "        \"\"\"Take an action and return the response of the env.\"\"\"\n",
    "        next_state, reward, done, _ = self.env.step(action)\n",
    "\n",
    "        if not self.is_test:\n",
    "            self.transition += [reward, next_state, done]\n",
    "            self.memory.store(*self.transition)\n",
    "    \n",
    "        return next_state, reward, done\n",
    "\n",
    "    def update_model(self) -> torch.Tensor:\n",
    "        \"\"\"Update the model by gradient descent.\"\"\"\n",
    "        samples = self.memory.sample_batch(self.beta)  # PER needs beta to calculate weights\n",
    "        weights = torch.FloatTensor(samples[\"weights\"].reshape(-1, 1)).to(self.device)\n",
    "        indices = samples[\"indices\"]\n",
    "\n",
    "        # PER: importance sampling before average\n",
    "        elementwise_loss = self._compute_dqn_loss(samples)  \n",
    "        loss = torch.mean(elementwise_loss * weights)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # gradient clipping\n",
    "        # https://pytorch.org/docs/stable/nn.html#torch.nn.utils.clip_grad_norm_\n",
    "        clip_grad_norm_(self.dqn.parameters(), 1.0, norm_type=1)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # PER: update priorities\n",
    "        loss_for_prior = elementwise_loss.detach().cpu().numpy()\n",
    "        new_priorities = loss_for_prior + self.prior_eps\n",
    "        self.memory.update_priorities(indices, new_priorities)\n",
    "        \n",
    "        # NoisyNet: reset noise\n",
    "        self.dqn.reset_noise()\n",
    "        self.dqn_target.reset_noise()\n",
    "\n",
    "        return loss.item()\n",
    "        \n",
    "    def train(self, num_frames: int, plotting_interval: int = 200):\n",
    "        \"\"\"Train the agent.\"\"\"\n",
    "        self.is_test = False\n",
    "        \n",
    "        state = self.env.reset()\n",
    "        update_cnt = 0\n",
    "        losses = []\n",
    "        scores = []\n",
    "        score = 0\n",
    "\n",
    "        for frame_idx in range(1, num_frames + 1):\n",
    "            action = self.select_action(state)\n",
    "            next_state, reward, done = self.step(action)\n",
    "\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            \n",
    "            # PER: increase beta\n",
    "            fraction = min(frame_idx / num_frames, 1.0)\n",
    "            self.beta = self.beta + fraction * (1.0 - self.beta)\n",
    "            \n",
    "            # NoisyNet: removed decrease of epsilon\n",
    "\n",
    "            # if episode ends\n",
    "            if done:\n",
    "                state = env.reset()\n",
    "                scores.append(score)\n",
    "                score = 0\n",
    "\n",
    "            # if training is ready\n",
    "            if len(self.memory) >= self.batch_size:\n",
    "                loss = self.update_model()\n",
    "                losses.append(loss)\n",
    "                update_cnt += 1\n",
    "                \n",
    "                # if hard update is needed\n",
    "                if update_cnt % self.target_update == 0:\n",
    "                    self._target_hard_update()\n",
    "\n",
    "            # plotting\n",
    "            if frame_idx % plotting_interval == 0:\n",
    "                self._plot(frame_idx, scores, losses)\n",
    "                \n",
    "        self.env.close()\n",
    "                \n",
    "    def test(self):\n",
    "        \"\"\"Test the agent.\"\"\"\n",
    "        self.is_test = True\n",
    "        \n",
    "        state = self.env.reset()\n",
    "        done = False\n",
    "        score = 0\n",
    "        \n",
    "        while not done:\n",
    "            self.env.render()\n",
    "            action = self.select_action(state)\n",
    "            next_state, reward, done = self.step(action)\n",
    "\n",
    "            state = next_state\n",
    "            score += reward\n",
    "        \n",
    "        print(\"score: \", score)\n",
    "        self.env.close()\n",
    "\n",
    "    def _compute_dqn_loss(self, samples: Dict[str, np.ndarray]) -> torch.Tensor:\n",
    "        \"\"\"Return dqn loss.\"\"\"\n",
    "        state = torch.FloatTensor(samples[\"obs\"]).to(self.device)\n",
    "        next_state = torch.FloatTensor(samples[\"next_obs\"]).to(self.device)\n",
    "        action = torch.LongTensor(samples[\"acts\"]).to(self.device)\n",
    "        reward = torch.FloatTensor(samples[\"rews\"].reshape(-1, 1)).to(self.device)\n",
    "        done = torch.FloatTensor(samples[\"done\"].reshape(-1, 1)).to(self.device)\n",
    "\n",
    "        # G_t   = r + gamma * v(s_{t+1})  if state != Terminal\n",
    "        #       = r                       otherwise\n",
    "        curr_q_value = self.dqn(state).gather(1, action)\n",
    "        next_q_value = self.dqn_target(next_state).gather(  # Double DQN\n",
    "            1, self.dqn(next_state).argmax(dim=1, keepdim=True)\n",
    "        ).detach()\n",
    "        mask = 1 - done\n",
    "        target = (reward + self.gamma * next_q_value * mask).to(self.device)\n",
    "\n",
    "        # calculate element-wise dqn loss\n",
    "        elementwise_loss = (target - curr_q_value).pow(2)\n",
    "\n",
    "        return elementwise_loss\n",
    "    \n",
    "    def _target_hard_update(self):\n",
    "        \"\"\"Hard update: target <- local.\"\"\"\n",
    "        self.dqn_target.load_state_dict(self.dqn.state_dict())\n",
    "                \n",
    "    def _plot(\n",
    "        self, \n",
    "        frame_idx: int, \n",
    "        scores: List[float], \n",
    "        losses: List[float], \n",
    "    ):\n",
    "        \"\"\"Plot the training progresses.\"\"\"\n",
    "        clear_output(True)\n",
    "        plt.figure(figsize=(20, 5))\n",
    "        plt.subplot(131)\n",
    "        plt.title('frame %s. score: %s' % (frame_idx, np.mean(scores[-10:])))\n",
    "        plt.plot(scores)\n",
    "        plt.subplot(132)\n",
    "        plt.title('loss')\n",
    "        plt.plot(losses)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment\n",
    "\n",
    "You can see the [code](https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py) and [configurations](https://github.com/openai/gym/blob/master/gym/envs/__init__.py#L53) of CartPole-v0 from OpenAI's repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# environment\n",
    "env_id = \"CartPole-v0\"\n",
    "env = gym.make(env_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set random seed\n",
    "\n",
    "The following is how to set the random seed for reproducibility, which is guided from [pytorch documents](https://pytorch.org/docs/stable/notes/randomness.html). Unfortunately, it doesn't work properly for now (pytorch 1.0.1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_torch(seed=777):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.backends.cudnn.enabled:\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "seed_torch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# parameters\n",
    "num_frames = 10000\n",
    "memory_size = 1000\n",
    "batch_size = 32\n",
    "target_update = 200\n",
    "\n",
    "# train\n",
    "agent = DQNAgent(env, memory_size, batch_size, target_update)\n",
    "agent.train(num_frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test\n",
    "\n",
    "Run the trained agent (1 episode)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score:  200.0\n"
     ]
    }
   ],
   "source": [
    "agent.test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
